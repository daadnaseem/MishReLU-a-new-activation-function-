{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.6.0","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dense, Dropout, Activation,Bidirectional \nfrom sklearn.metrics import f1_score, classification_report\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorboard.plugins.hparams import api as hp\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n#List all available activation functions in tf.keras.activations\nactivation_functions = dir(tf.keras.activations)\n# Filter out the built-in attributes\nactivation_functions = [func for func in activation_functions if not func.startswith('__')]\nprint(activation_functions)\n\n# Define the MishRelu activation function\ndef MishRelU(x):\n    return tf.where(x > 0, x, x * tf.keras.activations.tanh(tf.keras.activations.softplus(x)))\nget_custom_objects()['MishRelU'] = MishRelU\n# Example\nx = tf.convert_to_tensor([-5,2,4], dtype=tf.float32)\nMishRelU(x)\n\n#Define ReLU activation function\ndef ReLU(x):\n    return tf.keras.activations.relu(x)\nget_custom_objects()['ReLU'] = ReLU\n# Example\nx = tf.convert_to_tensor([-5,2,4], dtype=tf.float32)\nReLU(x)\n\n#Define Mish activation function\ndef Mish(x):\n    return x * tf.keras.activations.tanh(tf.keras.activations.softplus(x))\nget_custom_objects()['Mish'] = Mish\n# Example\nx = tf.convert_to_tensor([-5,2,4], dtype=tf.float32)\nMish(x)\n\n#Define Elu activation function\ndef Elu(x,a=1):\n    return  tf.keras.activations.elu(x, alpha=a)    \nget_custom_objects()['Elu'] = Elu\n# Example\nx = tf.convert_to_tensor([-5,2,4], dtype=tf.float32)\nElu(x)\n\n#Define LeakyReLU activation function\ndef LeakyReLU(x):\n    return tf.keras.layers.LeakyReLU(alpha=0.01)(x)\nget_custom_objects()['LeakyReLU'] = LeakyReLU\n# Example\nx = tf.convert_to_tensor([-5,2,4], dtype=tf.float32)\nLeakyReLU(x)\n\n#Define Softplus activation function\ndef Softplus(x):\n    return tf.keras.activations.softplus(x)\nget_custom_objects()['Softplus'] = Softplus\n# Example\nx = tf.convert_to_tensor([-5,2,4], dtype=tf.float32)\nSoftplus(x)\n\n#Define Tanh activation function\ndef Tanh(x):\n    return tf.keras.activations.tanh(x)\nget_custom_objects()['Tanh'] = Tanh\n# Example\nx = tf.convert_to_tensor([-5,2,4], dtype=tf.float32)\nTanh(x)\n\n#Define Selu activation function\ndef Selu(x):\n    return tf.keras.activations.selu(x)\nget_custom_objects()['Selu'] = Selu\n# Example\nx = tf.convert_to_tensor([-5,2,4], dtype=tf.float32)\nSelu(x)\n\n#BUILD CNN MODEL \ndef build_mnist_model_cnn3(activation, optimizer_name='adam', learning_rate=0.001):\n    model = Sequential()\n    model.add(Input(shape=(32, 32, 3)))\n    model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n    model.add(BatchNormalization())\n    model.add(layers.Activation(activation))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), padding='Same'))\n    model.add(BatchNormalization())\n    model.add(layers.Activation(activation))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding='Same'))\n    model.add(BatchNormalization())\n    model.add(layers.Activation(activation))\n    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(layers.Dense(512))\n    model.add(BatchNormalization())\n    model.add(layers.Activation(activation))\n    model.add(Dropout(0.5))\n    model.add(layers.Dense(10, activation='softmax'))\n\n    if optimizer_name.lower() == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n#Build MLP MODEL USEING FASHION-MNIST DATASET\ndef build_fashion_model(activation, optimizer_name='adam', learning_rate=0.001):\n    # Sequential \n    model = keras.Sequential()\n    model.add(layers.Input(shape=(28, 28)))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(units=398))\n    model.add(BatchNormalization())\n    model.add(layers.Activation(activation))\n    model.add(layers.Dropout(rate=0.25)) \n    \n    model.add(layers.Dense(units=128))\n    model.add(BatchNormalization())\n    model.add(layers.Activation(activation))\n    model.add(layers.Dropout(rate=0.25))\n    \n    model.add(layers.Dense(units=64))\n    model.add(BatchNormalization())\n    model.add(layers.Activation(activation))\n    model.add(layers.Dropout(rate=0.25))\n    # Output layer\n    model.add(layers.Dense(units=10, activation='softmax'))\n    #print(model.summary())\n    # Select optimizer \n    if optimizer_name.lower() == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    else:\n        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\n# LSTM MODEL BUILDING\ndef build_lstm_model(activation, learning_rate):\n    model = Sequential()\n    model.add(Embedding(input_dim =5000, output_dim = 128, input_length = 200))\n    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(32, kernel_regularizer=regularizers.l2(0.001)))\n    model.add(Activation(activation))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    # Use Adam optimizer \n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n    \n# LSTM MODEL BUILDING\ndef BiLSTM_model(activation, learning_rate):\n    model = Sequential()\n    model.add(Embedding(input_dim =10000, output_dim =32,input_length=200))\n    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n    model.add(Dropout(0.4))\n    model.add(Dense(32, kernel_regularizer=regularizers.l2(0.001)))\n    model.add(Activation(activation))\n    model.add(Dropout(0.5))\n    model.add(Dense(46, activation='softmax')) #num_classes=\n    # Use Adam optimizer \n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    #model.compile(optimizer = \"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\nlr_schedule = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, factor=0.2, min_lr=1e-5)\n\nactivation_functions = [MishRelU, RelU, Mish, LeakyReLU]\n# activation_functions = [Tanh, Softplus, Sigmoid, Selu]\nlearning_rates = [0.1, 0.01, 0.001, 0.0001]\noptimizers = ['Adam']\nMETRIC_ACCURACY = 'accuracy'\nbase_log_dir = \"logs_relu/gridsearch\"\n\nfor activation in activation_functions:\n    act_name = activation.__name__\n    \n    for optimizer_name in optimizers:\n        for learning_rate in learning_rates:\n            start = time.time()\n            log_dir = f\"{base_log_dir}/{act_name}_{optimizer_name}_lr{learning_rate}\"\n            writer = tf.summary.create_file_writer(log_dir)\n            tensorboard = TensorBoard(\n                log_dir=log_dir,\n                histogram_freq=1,\n                write_graph=True,\n                write_images=False\n            )\n            print(f\"Training with {act_name}, {optimizer_name}, lr={learning_rate}\")\n            # Build model\n            model = build_mnist_model_cnn3(\n                activation,\n                optimizer_name=optimizer_name,\n                learning_rate=learning_rate\n            )\n            callbacks = [tensorboard, lr_schedule]\n            \n            # Train model\n            history = model.fit(\n                X_train, y_train,\n                epochs=30,\n                validation_data=(X_val, y_val),\n                batch_size=128,\n                callbacks=callbacks\n            )\n\n            # Evaluation\n            test_loss, test_acc = model.evaluate(X_test, y_test)\n\n            # Predictions\n            y_pred_probs = model.predict(X_test)\n            y_pred = np.argmax(y_pred_probs, axis=1)\n            y_true = np.argmax(y_test, axis=1)\n\n            # F1 Score\n            f1 = f1_score(y_true, y_pred, average='macro')\n            print(\"F1-score (macro):\", f1)\n            print(classification_report(y_true, y_pred))\n\n            # Timing\n            end = time.time()\n            total_minutes = (end - start) / 60\n            print(f\"Test Accuracy: {test_acc:.4f} | Time taken: {total_minutes:.2f} min for {act_name}, optimizer={optimizer_name}, learning_rate={learning_rate}\")\n\n            # Save model\n            model_path = f\"{log_dir}/model.keras\"\n            model.save(model_path)\n\n            # Model size\n            size_bytes = os.path.getsize(model_path)\n            size_mb = size_bytes / (1024 * 1024)\n            print(f\"Model size (activation={act_name}): {size_mb:.2f} MB\")\n\nprint('Training complete!')\n\n    ","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":false},"outputs":[],"execution_count":null}]}