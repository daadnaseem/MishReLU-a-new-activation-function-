{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.6.0","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# Import required libraries for data processing, modeling,\n# evaluation metrics, statistical tests, and reproducibility\n# ============================================================\nimport os, time, random\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom scipy.stats import ttest_rel\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Activation,Bidirectional \nfrom sklearn.metrics import f1_score, classification_report\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorboard.plugins.hparams import api as hp\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.datasets import mnist, fashion_mnist, cifar100\nfrom scipy.stats import friedmanchisquare\nfrom scipy.stats import wilcoxon\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, Add, Input, Flatten, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.datasets import reuters\nfrom tensorflow.keras.utils import to_categorical\n","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n# ============================================================\n# Dataset Loading and Preprocessing\n# ============================================================\n","metadata":{}},{"cell_type":"code","source":"# ======================================================================================\n# MNIST Dataset Preparation:\n# 1. Load the dataset.\n# 2. Normalize pixel values to [0, 1] \n# 3. Reshape data to (Samples, Height, Width, Channels) for CNN compatibility.\n# 4. Convert integer labels to One-Hot encoded vectors \n# 5. Split training data to create a Validation set for Early Stopping monitoring.\n# ======================================================================================\n\n# Load the dataset \n(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n\n# Normalize pixel \nX_train_mnist = X_train_mnist.astype(\"float32\") / 255.0\nX_test_mnist  = X_test_mnist.astype(\"float32\") / 255.0\n\n# Reshape \nX_train_mnist = X_train_mnist.reshape(-1, 28, 28, 1)\nX_test_mnist  = X_test_mnist.reshape(-1, 28, 28, 1)\n\n# One-hot encode \ny_train_mnist = to_categorical(y_train_mnist, 10)\ny_test_mnist  = to_categorical(y_test_mnist, 10)\n\n# split\n\nX_train_mnist, X_val_mnist, y_train_mnist, y_val_mnist = train_test_split(\n    X_train_mnist, y_train_mnist, \n    test_size=0.2, \n    random_state=42, \n    stratify=y_train_mnist\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =======  Fashion-MNIST Dataset  ======","metadata":{}},{"cell_type":"code","source":"# ======================================================================================\n# Fashion-MNIST Dataset Preparation:\n# 1. Load the dataset \n# 2. Normalize pixel values to the range [0, 1]\n# 3. Reshape images to include the channel dimension (Height, Width, Channels).\n# 4. Perform One-Hot Encoding on labels for multi-class classification.\n# ======================================================================================\n\n# Load the dataset \n(X_train_fm, y_train_fm), (X_test_fm, y_test_fm) = fashion_mnist.load_data()\n\n# Scale pixel values \nX_train_fm = X_train_fm.astype(\"float32\") / 255.0\nX_test_fm  = X_test_fm.astype(\"float32\") / 255.0\n\n# Reshape data to (Samples, 28, 28, 1) \nX_train_fm = X_train_fm.reshape(-1, 28, 28, 1)\nX_test_fm  = X_test_fm.reshape(-1, 28, 28, 1)\n\n# Convert integer class labels to binary class matrices \ny_train_fm = to_categorical(y_train_fm, 10)\ny_test_fm  = to_categorical(y_test_fm, 10)\n\n\n\n# Create a Validation split \nX_train_fm, X_val_fm, y_train_fm, y_val_fm = train_test_split(\n    X_train_fm, y_train_fm, \n    test_size=0.2, \n    random_state=42, \n    stratify=y_train_fm\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =======  CIFAR-100  ======","metadata":{}},{"cell_type":"code","source":"\n# =============================\n# 3. CIFAR-100\n# 1. Load Data\n# 2. Spilt CIFAR-100 DATA SET TO Train and Val \n# 3. Data Augmentation For CIFAR-100 Dataset\n# =============================\n(X_train_c100, y_train_c100), (X_test_c100, y_test_c100) = cifar100.load_data(label_mode='fine')\n\nX_train_c100 = X_train_c100.astype(\"float32\") / 255.0\nX_test_c100  = X_test_c100.astype(\"float32\") / 255.0\n\ny_train_c100 = to_categorical(y_train_c100, 100)\ny_test_c100 =  to_categorical(y_test_c100, 100)\n\n#=== Spilt CIFAR-100 DATA SET TO Train and Test \nx_train_c100, x_val_c100, y_train_c100, y_val_c100 = train_test_split(\n    x_train_c100, y_train_c100,\n    test_size=0.2,\n    random_state=42\n)\n\n#===Data Augmentation Setup For CIFAR-100 Dataset:\n#This helps prevent overfitting and improves the model's ability to generalize to new images.\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    zoom_range=0.1\n)\n\n#Calculates necessary statistics (like mean/std) from the training data to apply feature-wise normalization if required.\ndatagen.fit(x_train_c100)\n\n# Create a generator for training that delivers transformed images in batches\ntrain_generator_c100 = datagen.flow(\n    x_train_c100,\n    y_train_c100,\n    batch_size=256\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# =======  IMDB Dataset  ======","metadata":{}},{"cell_type":"code","source":"# ======================================================================================\n# IMDB Dataset Preparation Pipeline:\n# 1. Load data and encode labels (Positive: 1, Negative: 0).\n# 2. Split the raw data into initial Training and Testing sets.\n# 3. Tokenize text data to convert words into numerical sequences .\n# 4. Pad sequences to ensure a uniform input length of 200 for the Neural Network.\n# 5. Perform a second split to create a Validation set from the training data.\n# ======================================================================================\n\n# Load the dataset\nimdb_data = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\n# Encode categorical labels into numerical format\n# positive -> 1\n# negative -> 0\nimdb_data.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)\n\n# First Split: Separate the data into Training and Testing sets (80/20)\ntrain_imdb_data, test_imdb_data = train_test_split(\n    imdb_data, test_size=0.2, random_state=42, stratify=imdb_data[\"sentiment\"]\n)\n\n# Initialize Tokenizer \ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(train_imdb_data[\"review\"])\n\n# Convert text reviews to numerical sequences and apply padding\nx_train_imdb_data = pad_sequences(tokenizer.texts_to_sequences(train_imdb_data[\"review\"]), maxlen=200)\nx_test_imdb_data = pad_sequences(tokenizer.texts_to_sequences(test_imdb_data[\"review\"]), maxlen=200)\n\n# Convert labels to NumPy arrays for compatibility with TensorFlow\ny_train_imdb_data = train_imdb_data[\"sentiment\"].to_numpy()\ny_test_imdb_data = test_imdb_data[\"sentiment\"].to_numpy()\n\n# Second Split: Reserve 20% of the training data for Validation\nx_train_imdb_data, x_val_imdb_data, y_train_imdb_data, y_val_imdb_data = train_test_split(\n    x_train_imdb_data, y_train_imdb_data, \n    test_size=0.2, random_state=42, stratify=y_train_imdb_data\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================================\n# Reuters Newswire Dataset Preparation:\n# 1. Load the dataset ( Labels are 46 different topics.\n# 2. Pad sequences to ensure a uniform length of 200 for Neural Network input.\n# 3. Convert labels to One-Hot encoded vectors.\n# 4. Split training data to create a Validation set for monitoring training.\n# ======================================================================================\n\n# Load the datase\n(x_train_reuters, y_train_reuters), (x_test_reuters, y_test_reuters) = reuters.load_data(num_words=10000)\n\n#Pad sequences \nmax_words = 200 \nx_train_reuters = sequence.pad_sequences(x_train_reuters, maxlen=max_words)\nx_test_reuters = sequence.pad_sequences(x_test_reuters, maxlen=max_words)\n\n# One-hot encode the 46 categorical labels\ny_train_reuters = to_categorical(y_train_reuters, 46)\ny_test_reuters = to_categorical(y_test_reuters, 46)\n\n# Create a Validation split (20% of training data) \nx_train_reuters, x_val_reuters, y_train_reuters, y_val_reuters = train_test_split(\n    x_train, y_train, \n    test_size=0.2, \n    random_state=42, \n    stratify=y_train_reuters\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# List all available built-in activation functions in TensorFlow\n# ============================================================\n\nactivation_functions = dir(tf.keras.activations)\n# Filter out the built-in attributes\nactivation_functions = [func for func in activation_functions if not func.startswith('__')]\nprint(activation_functions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================================\n# Definition and registration of custom and baseline activation functions used in the experiments\n# ============================================================","metadata":{}},{"cell_type":"code","source":"# Define the MishRelu activation function \n\ndef MishRelU(x):\n    return tf.where(x > 0, x, x * tf.keras.activations.tanh(tf.keras.activations.softplus(x)))\nget_custom_objects()['MishRelU'] = MishRelU\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define Baseline ReLU activation function\n\ndef ReLU(x):\n    return tf.keras.activations.relu(x)\nget_custom_objects()['ReLU'] = ReLU\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define Baseline Mish activation function\ndef Mish(x):\n    return x * tf.keras.activations.tanh(tf.keras.activations.softplus(x))\nget_custom_objects()['Mish'] = Mish","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define Baseline Elu activation function\ndef Elu(x,a=1):\n    return  tf.keras.activations.elu(x, alpha=a)    \nget_custom_objects()['Elu'] = Elu\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define  Baseline LeakyReLU activation function\ndef LeakyReLU(x):\n    return tf.keras.layers.LeakyReLU(alpha=0.01)(x)\nget_custom_objects()['LeakyReLU'] = LeakyReLU\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define Baseline Selu activation function\ndef Selu(x):\n    return tf.keras.activations.selu(x)\nget_custom_objects()['Selu'] = Selu\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================================\n# Reproducibility Function:\n# and the operating system environment. \n# ======================================================================================\ndef set_seed(seed=42):\n    # Fix the hash seed for consistent dictionary/set behavior\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n    # Fix the seed for Python's built-in random module\n    random.seed(seed)\n    \n    # Fix the seed for NumPy's numerical operations\n    np.random.seed(seed)\n    \n    # Fix the seed for TensorFlow/Keras operations (Initializers, Shuffling, etc.)\n    tf.random.set_seed(seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Reproducibility setup\n# Random seeds \n## This function fixes the random seeds across all libraries (Python, NumPy, TensorFlow) \n# By setting a constant seed, we ensure that  every time the code runs, it produces identical results\n# ============================================================\ndef set_seed(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================\n# Multi-Layer Perceptron (MLP) model for Fashion-MNIST\n# ============================================================","metadata":{}},{"cell_type":"code","source":"def build_fashion_model_MLP(activation, optimizer_name='adam', learning_rate=0.001):\n    # Sequential model\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(28, 28)))\n    model.add(layers.Flatten())\n\n    model.add(layers.Dense(units=398))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.Dropout(rate=0.1)) \n    \n    model.add(layers.Dense(units=128))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.Dropout(rate=0.1))\n    \n    model.add(layers.Dense(units=64))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.Dropout(rate=0.1))\n    \n    # Output layer\n    model.add(layers.Dense(units=10, activation='softmax'))\n\n    # --------- اختيار الـ Optimizer ---------\n    optimizer_name = optimizer_name.lower()\n    \n    if optimizer_name == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    elif optimizer_name == 'nadam':\n        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\n    # Compile the model\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================================\n# Convolutional Neural Network (CNN) model for MNIST\n# ============================================================","metadata":{}},{"cell_type":"code","source":"\n\n\ndef build_mnist_model_cnn3(activation, optimizer_name='adam', learning_rate=0.001):\n    model = Sequential()\n\n    # Input layer\n    model.add(Input(shape=(28, 28, 1)))\n\n    model.add(layers.Conv2D(32, (3,3), padding='same'))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.MaxPooling2D((2,2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(layers.Conv2D(64, (3,3), padding='same'))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.MaxPooling2D((2,2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(layers.Conv2D(128, (3,3), padding='same'))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.MaxPooling2D((2,2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(layers.Dense(512))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(layers.Dense(10, activation='softmax'))\n\n    # --------- اختيار الـ Optimizer ---------\n    optimizer_name = optimizer_name.lower()\n\n    if optimizer_name == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n    elif optimizer_name == 'rmsprop':\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n\n    elif optimizer_name == 'nadam':\n        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\n    # compile model\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ======= ResNet-18 architecture adapted for CIFAR-100 dataset ========\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\nfrom tensorflow.keras.models import Model\n\n\ndef resnet_block(x, filters,activation_fc='relu', stride=1):\n    shortcut = x\n\n    x = Conv2D(filters, 3, strides=stride, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation_fc)(x)\n\n    x = Conv2D(filters, 3, strides=1, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n\n    if stride != 1 or shortcut.shape[-1] != filters:\n        shortcut = Conv2D(filters, 1, strides=stride, padding='same', use_bias=False)(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation(activation_fc)(x)\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ResNet18_CIFAR(input_shape=(32,32,3), num_classes=100, activation_fc='relu'):\n    inputs = Input(shape=input_shape)\n\n    x = Conv2D(64, 3, padding='same', use_bias=False)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(activation_fc)(x)\n\n    x = resnet_block(x, 64, activation_fc=activation_fc)\n    x = resnet_block(x, 64, activation_fc=activation_fc)\n\n    x = resnet_block(x, 128, activation_fc=activation_fc, stride=2)\n    x = resnet_block(x, 128, activation_fc=activation_fc)\n\n    x = resnet_block(x, 256, activation_fc=activation_fc, stride=2)\n    x = resnet_block(x, 256, activation_fc=activation_fc)\n\n    x = resnet_block(x, 512, activation_fc=activation_fc, stride=2)\n    x = resnet_block(x, 512, activation_fc=activation_fc)\n\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation=activation_fc)(x)\n    outputs = Dense(num_classes, activation='softmax', dtype='float32')(x)\n\n    return Model(inputs, outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(\n    input_shape=(32,32,3),\n    activation_fc='relu',\n    learning_rate=0.001,\n    num_classes=100):\n\n    model = ResNet18_CIFAR(\n        input_shape=input_shape,\n        num_classes=num_classes,\n        activation_fc=activation_fc\n    )\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================================================================================\n# Early Stopping Callback:\n# This mechanism prevents overfitting by monitoring the model's performance on the \n# validation set.\n# ======================================================================================\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop_cifar100 = EarlyStopping(\n    monitor='val_loss',       # Monitor the validation loss to detect the stagnation point\n    patience=5,               # Number of epochs to wait before stopping if no improvement occurs\n    restore_best_weights=True # Automatically rollback to the weights that gave the lowest val_loss\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ======= LSTM MODEL For IMDB Dataset =======","metadata":{}},{"cell_type":"code","source":"# LSTM MODEL BUILDING\ndef build_lstm_model(activation, optimizer_name,learning_rate):\n    model = Sequential()\n    model.add(Embedding(input_dim =5000, output_dim = 128, input_length = 200))\n    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(32, activation=activation, kernel_regularizer=regularizers.l2(0.001)))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Use Adam optimizer \n    # --------- اختيار الـ Optimizer ---------\n    optimizer_name = optimizer_name.lower()\n    \n    if optimizer_name == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    elif optimizer_name == 'adamw':\n        optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-4)\n    elif optimizer_name == 'rmsprop':\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n    elif optimizer_name == 'nadam':\n        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n    elif optimizer_name == 'radam':\n        try:\n            optimizer = tfa.optimizers.RectifiedAdam(learning_rate=learning_rate)\n        except:\n            raise ValueError(\"RAdam requires tensorflow-addons installed.\")\n    elif optimizer_name == 'sgd':\n        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BiLSTM MODEL ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, Activation\nfrom tensorflow.keras import regularizers\n\n\ndef BiLSTM_model(activation, optimizer_name, learning_rate):\n    # بناء النموذج\n    model = Sequential()\n    model.add(Embedding(input_dim=10000, output_dim=32, input_length=200))\n    model.add(Bidirectional(LSTM(32, return_sequences=False)))\n    model.add(Dropout(0.5))\n    model.add(Dense(32, kernel_regularizer=regularizers.l2(0.001)))\n    model.add(Activation(activation))\n    model.add(Dropout(0.2))\n    model.add(Dense(46, activation='softmax'))  # num_classes=46\n\n    # تحديد المحسن Optimizer\n    optimizer_name = optimizer_name.lower()\n    if optimizer_name == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    elif optimizer_name == 'adamw':\n        optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-4)\n    elif optimizer_name == 'rmsprop':\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n    elif optimizer_name == 'nadam':\n        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n    elif optimizer_name == 'radam':\n        try:\n            optimizer = tfa.optimizers.RectifiedAdam(learning_rate=learning_rate)\n        except:\n            raise ValueError(\"RAdam requires tensorflow-addons installed.\")\n    elif optimizer_name == 'sgd':\n        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\n    # Compile the model\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ====================================\n# Model training and evaluation\n# - Multiple random seeds\n# - Different learning rates\n# - Mean and standard deviation are reported\n# ====================================","metadata":{}},{"cell_type":"code","source":"# List of predefined random seeds for reproducibility\nSEEDS = [164, 343, 865, 599, 251]\n\n#to store metrics (Accuracy, Loss, etc.)\nall_results = [] \n\nactivation_functions = [MishRelU, ReLU, Mish, Elu, LeakyReLU, Selu]\nlearning_rates = [0.01,0.001,0.0001]\noptimizers = [\"nadam\"]\n\nfor activation in activation_functions:\n    act_name = activation.__name__\n\n    for optimizer_name in optimizers:\n        for learning_rate in learning_rates:\n\n            print(f\"\\n>>> Training with {act_name}, {optimizer_name}, lr={learning_rate}\")\n            print(\"Seeds for this run:\", SEEDS)\n\n            accs = []\n            precisions = []\n            recalls = []\n            f1s = []\n\n            start = time.time()\n\n            for seed in SEEDS:\n                set_seed(seed)\n\n                #BUILD MODEL\n                model = build_mnist_model_cnn3(\n                    activation,\n                    optimizer_name=optimizer_name,\n                    learning_rate=learning_rate\n                )\n\n                # TRAUNING\n                history = model.fit(\n                    X_train_mnist, y_train_mnist,\n                    epochs=30,\n                    validation_split=0.2, \n                    batch_size=60,\n                    verbose=0,\n                    #callbacks=[early_stop]\n                )\n\n                # التقييم accuracy\n                test_loss, test_acc = model.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n                accs.append(test_acc)\n\n                # حساب precision, recall, f1\n                y_pred_probs = model.predict(X_test_mnist, verbose=0)\n                y_pred = np.argmax(y_pred_probs, axis=1)\n                y_true = np.argmax(y_test_mnist, axis=1)\n\n                precisions.append(precision_score(y_true, y_pred, average='macro'))\n                recalls.append(recall_score(y_true, y_pred, average='macro'))\n                f1s.append(f1_score(y_true, y_pred, average='macro'))\n\n            end = time.time()\n            total_minutes = (end - start) / 60\n\n            # المتوسط والانحراف\n            mean_acc = np.mean(accs)\n            std_acc = np.std(accs)\n\n            mean_prec = np.mean(precisions)\n            std_prec = np.std(precisions)\n\n            mean_rec = np.mean(recalls)\n            std_rec = np.std(recalls)\n\n            mean_f1 = np.mean(f1s)\n            std_f1 = np.std(f1s)\n\n            print(f\"{act_name}, lr={learning_rate} | \"\n                  f\"Acc: {mean_acc:.4f}±{std_acc:.4f} | \"\n                  f\"Prec: {mean_prec:.4f}±{std_prec:.4f} | \"\n                  f\"Rec: {mean_rec:.4f}±{std_rec:.4f} | \"\n                  f\"F1: {mean_f1:.4f}±{std_f1:.4f} \"\n                  f\"({len(SEEDS)} seeds) | Time: {total_minutes:.2f} min\")\n\n            all_results.append({\n                \"activation\": act_name,\n                \"optimizer\": optimizer_name,\n                \"lr\": learning_rate,\n                \"mean_acc\": mean_acc,\n                \"std_acc\": std_acc,\n                \"mean_prec\": mean_prec,\n                \"std_prec\": std_prec,\n                \"mean_rec\": mean_rec,\n                \"std_rec\": std_rec,\n                \"mean_f1\": mean_f1,\n                \"std_f1\": std_f1,\n                \"all_accs\": accs,\n                \"all_precs\": precisions,\n                \"all_recs\": recalls,\n                \"all_f1s\": f1s\n            })\n\n\n# ============================================================\n# Aggregate results across runs and convert to DataFrame\n# ============================================================\n\ndf_results = pd.DataFrame(all_results)\nprint(\"\\n=== Summary Results ===\")\nprint(df_results[[\n    \"activation\",\"lr\",\n    \"mean_acc\",\"std_acc\",\n    \"mean_prec\",\"std_prec\",\n    \"mean_rec\",\"std_rec\",\n    \"mean_f1\",\"std_f1\"\n]])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================\n# Statistical significance analysis\n# Friedman test followed by post-hoc Wilcoxon test\n# ============================================================","metadata":{}},{"cell_type":"code","source":"\nactivations = [\"MishRelU\", \"ReLU\", \"Mish\", \"Elu\", \"LeakyReLU\", \"Selu\"]\nlearning_rates = [0.01, 0.001, 0.0001]\n\nmetrics = [\"all_accs\", \"all_precs\", \"all_recs\", \"all_f1s\"]\n\nfor lr in learning_rates:\n    print(f\"\\n=== Friedman Test for lr = {lr} ===\")\n    for metric in metrics:\n        #\n        values = [df_results.query(f\"activation=='{act}' and lr=={lr}\")[metric].values[0] for act in activations]\n        stat, p = friedmanchisquare(*values)\n        print(f\"{metric.replace('all_', '').capitalize()} -> Friedman statistic = {stat:.4f}, p-value = {p:.6f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#========== Post-hoc Wilcoxon Test ============= ","metadata":{}},{"cell_type":"code","source":"activations = [\"MishRelU\", \"ReLU\", \"Mish\", \"Elu\", \"LeakyReLU\", \"Selu\"]\nmetrics = [\"all_accs\", \"all_precs\", \"all_recs\", \"all_f1s\"]\nlr = 0.0001  \n\ndef get_values(act, metric):\n    return df_results.query(f\"activation=='{act}' and lr=={lr}\")[metric].values[0]\n\nfor metric in metrics:\n    print(\"\\n===================================\")\n    print(\"Post-hoc Wilcoxon Test | Metric:\", metric.replace(\"all_\",\"\").upper())\n    print(\"===================================\\n\")\n\n    # Collect the performance values for each activation function\n    data = {act: get_values(act, metric) for act in activations}\n\n    # Compute the mean performance to rank activation functions\n    avg_scores = {act: np.mean(vals) for act, vals in data.items()}\n\n    print(\"Average Scores:\")\n    for act, score in avg_scores.items():\n        print(f\"{act}: {score:.4f}\")\n\n    # the best function\n    best_act = max(avg_scores, key=avg_scores.get)\n    print(f\"\\n==> BEST activation (based on mean): {best_act}\\n\")\n\n    # Wilcoxon between each pair\n    print(\"Wilcoxon Pairwise p-values:\")\n    for i in range(len(activations)):\n        for j in range(i+1, len(activations)):\n            act1 = activations[i]\n            act2 = activations[j]\n            stat, p = wilcoxon(data[act1], data[act2])\n            print(f\"{act1} vs {act2}: p = {p:.6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}