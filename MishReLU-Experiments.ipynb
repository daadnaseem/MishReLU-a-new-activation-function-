{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.6.0","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# Import required libraries for data processing, modeling,\n# evaluation metrics, statistical tests, and reproducibility\n# ============================================================\nimport os, time, random\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom scipy.stats import ttest_rel\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Activation,Bidirectional \nfrom sklearn.metrics import f1_score, classification_report\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorboard.plugins.hparams import api as hp\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.datasets import mnist, fashion_mnist, cifar100\nfrom scipy.stats import friedmanchisquare\nfrom scipy.stats import wilcoxon\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, Add, Input, Flatten, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n# ============================================================\n# Dataset Loading and Preprocessing\n# Datasets are loaded from standard Keras libraries to ensure\n# accessibility and full reproducibility\n# ============================================================","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# MNIST and Fashion-Mnist Datasets preprocessing\n# - Normalization to [0,1]\n# - Reshaping for CNN input\n# - One-hot encoding of labels\n# -----------------------------\n# 1- MNIST\n\n(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n\nX_train_mnist = X_train_mnist.astype(\"float32\") / 255.0\nX_test_mnist  = X_test_mnist.astype(\"float32\") / 255.0\n\nX_train_mnist = X_train_mnist.reshape(-1, 28, 28, 1)\nX_test_mnist  = X_test_mnist.reshape(-1, 28, 28, 1)\n\ny_train_mnist = to_categorical(y_train_mnist, 10)\ny_test_mnist  = to_categorical(y_test_mnist, 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =============================\n# 2. Fashion-MNIST\n# =============================\n(X_train_fm, y_train_fm), (X_test_fm, y_test_fm) = fashion_mnist.load_data()\n\nX_train_fm = X_train_fm.astype(\"float32\") / 255.0\nX_test_fm  = X_test_fm.astype(\"float32\") / 255.0\n\nX_train_fm = X_train_fm.reshape(-1, 28, 28, 1)\nX_test_fm  = X_test_fm.reshape(-1, 28, 28, 1)\n\ny_train_fm = to_categorical(y_train_fm, 10)\ny_test_fm  = to_categorical(y_test_fm, 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# =============================\n# 3. CIFAR-100\n# =============================\n(X_train_c100, y_train_c100), (X_test_c100, y_test_c100) = cifar100.load_data(label_mode='fine')\n\nX_train_c100 = X_train_c100.astype(\"float32\") / 255.0\nX_test_c100  = X_test_c100.astype(\"float32\") / 255.0\n\ny_train_c100 = to_categorical(y_train_c100, 100)\n\n\n# ============================================================\n# Display dataset shapes for verification\n# ============================================================\nprint(\"MNIST:\", X_train_mnist.shape, y_train_mnist.shape)\nprint(\"Fashion-MNIST:\", X_train_fm.shape, y_train_fm.shape)\nprint(\"CIFAR-100:\", X_train_c100.shape, y_train_c100.shape)\n#==================================================================","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# List all available built-in activation functions in TensorFlow\n# ============================================================\n\nactivation_functions = dir(tf.keras.activations)\n# Filter out the built-in attributes\nactivation_functions = [func for func in activation_functions if not func.startswith('__')]\nprint(activation_functions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Definition and registration of custom and baseline\n# activation functions used in the experiments\n# ============================================================\n# Define the MishRelu activation function\n# Custom MishReLU activation function\n\ndef MishRelU(x):\n    return tf.where(x > 0, x, x * tf.keras.activations.tanh(tf.keras.activations.softplus(x)))\nget_custom_objects()['MishRelU'] = MishRelU\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define Baseline ReLU activation function\n\ndef ReLU(x):\n    return tf.keras.activations.relu(x)\nget_custom_objects()['ReLU'] = ReLU\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define Baseline Mish activation function\ndef Mish(x):\n    return x * tf.keras.activations.tanh(tf.keras.activations.softplus(x))\nget_custom_objects()['Mish'] = Mish","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define Baseline Elu activation function\ndef Elu(x,a=1):\n    return  tf.keras.activations.elu(x, alpha=a)    \nget_custom_objects()['Elu'] = Elu\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define  Baseline LeakyReLU activation function\ndef LeakyReLU(x):\n    return tf.keras.layers.LeakyReLU(alpha=0.01)(x)\nget_custom_objects()['LeakyReLU'] = LeakyReLU\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Define Baseline Selu activation function\ndef Selu(x):\n    return tf.keras.activations.selu(x)\nget_custom_objects()['Selu'] = Selu\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# Reproducibility setup\n# Random seeds \n# ============================================================\ndef set_seed(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================\n# Multi-Layer Perceptron (MLP) model for Fashion-MNIST\n# ============================================================","metadata":{}},{"cell_type":"code","source":"def build_fashion_model_MLP(activation, optimizer_name='adam', learning_rate=0.001):\n    # Sequential model\n    model = keras.Sequential()\n    model.add(layers.Input(shape=(28, 28)))\n    model.add(layers.Flatten())\n\n    model.add(layers.Dense(units=398))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.Dropout(rate=0.1)) \n    \n    model.add(layers.Dense(units=128))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.Dropout(rate=0.1))\n    \n    model.add(layers.Dense(units=64))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.Dropout(rate=0.1))\n    \n    # Output layer\n    model.add(layers.Dense(units=10, activation='softmax'))\n\n    # --------- اختيار الـ Optimizer ---------\n    optimizer_name = optimizer_name.lower()\n    \n    if optimizer_name == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    elif optimizer_name == 'nadam':\n        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\n    # Compile the model\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================================\n# Convolutional Neural Network (CNN) model for MNIST\n# ============================================================","metadata":{}},{"cell_type":"code","source":"\n\n\ndef build_mnist_model_cnn3(activation, optimizer_name='adam', learning_rate=0.001):\n    model = Sequential()\n\n    # Input layer\n    model.add(Input(shape=(28, 28, 1)))\n\n    model.add(layers.Conv2D(32, (3,3), padding='same'))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.MaxPooling2D((2,2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(layers.Conv2D(64, (3,3), padding='same'))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.MaxPooling2D((2,2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(layers.Conv2D(128, (3,3), padding='same'))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(layers.MaxPooling2D((2,2), strides=2))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(layers.Dense(512))\n    model.add(layers.Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(layers.Dense(10, activation='softmax'))\n\n    # --------- اختيار الـ Optimizer ---------\n    optimizer_name = optimizer_name.lower()\n\n    if optimizer_name == 'adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n    elif optimizer_name == 'rmsprop':\n        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n\n    elif optimizer_name == 'nadam':\n        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n    else:\n        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n\n    # compile model\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#================== ResNet-18 block =============================\n# ResNet-18 architecture adapted for CIFAR-100 dataset\n# ============================================================\n","metadata":{}},{"cell_type":"code","source":"def resnet_block(x, filters, stride=1):\n    shortcut = x\n\n    x = Conv2D(filters, 3, strides=stride, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters, 3, strides=1, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n\n    if stride != 1 or shortcut.shape[-1] != filters:\n        shortcut = Conv2D(filters, 1, strides=stride, padding='same', use_bias=False)(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation('relu')(x)\n    return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ResNet18_CIFAR(input_shape=(32,32,3), num_classes=100, activation_fc='relu'):\n    inputs = Input(shape=input_shape)\n\n    x = Conv2D(64, 3, padding='same', use_bias=False)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # 8 basic blocks = ResNet-18\n    x = resnet_block(x, 64)\n    x = resnet_block(x, 64)\n\n    x = resnet_block(x, 128, stride=2)\n    x = resnet_block(x, 128)\n\n    x = resnet_block(x, 256, stride=2)\n    x = resnet_block(x, 256)\n\n    x = resnet_block(x, 512, stride=2)\n    x = resnet_block(x, 512)\n\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(128, activation=activation_fc)(x)\n    outputs = Dense(num_classes, activation='softmax', dtype='float32')(x)\n\n    return Model(inputs, outputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_model(\n    input_shape=(32,32,3),\n    activation_fc='relu',\n    learning_rate=0.001,\n    num_classes=100):\n\n    model = ResNet18_CIFAR(\n        input_shape=input_shape,\n        num_classes=num_classes,\n        activation_fc=activation_fc\n    )\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ====================================\n# Model training and evaluation\n# - Multiple random seeds\n# - Different learning rates\n# - Mean and standard deviation are reported\n# ====================================","metadata":{}},{"cell_type":"code","source":"SEEDS = [random.randint(0, 10000) for _ in range(3)]\nall_results = []  # تخزين كل النتائج\n\nactivation_functions = [MishRelU, ReLU, Mish, Elu, LeakyReLU, Selu]\nlearning_rates = [0.01,0.001,0.0001]\noptimizers = [\"nadam\"]\n\nfor activation in activation_functions:\n    act_name = activation.__name__\n\n    for optimizer_name in optimizers:\n        for learning_rate in learning_rates:\n\n            print(f\"\\n>>> Training with {act_name}, {optimizer_name}, lr={learning_rate}\")\n            print(\"Seeds for this run:\", SEEDS)\n\n            accs = []\n            precisions = []\n            recalls = []\n            f1s = []\n\n            start = time.time()\n\n            for seed in SEEDS:\n                set_seed(seed)\n\n                #BUILD MODEL\n                model = build_mnist_model_cnn3(\n                    activation,\n                    optimizer_name=optimizer_name,\n                    learning_rate=learning_rate\n                )\n\n                # TRAUNING\n                history = model.fit(\n                    X_train_mnist, y_train_mnist,\n                    epochs=30,\n                    validation_split=0.2, \n                    batch_size=60,\n                    verbose=0,\n                    #callbacks=[early_stop]\n                )\n\n                # التقييم accuracy\n                test_loss, test_acc = model.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n                accs.append(test_acc)\n\n                # حساب precision, recall, f1\n                y_pred_probs = model.predict(X_test_mnist, verbose=0)\n                y_pred = np.argmax(y_pred_probs, axis=1)\n                y_true = np.argmax(y_test_mnist, axis=1)\n\n                precisions.append(precision_score(y_true, y_pred, average='macro'))\n                recalls.append(recall_score(y_true, y_pred, average='macro'))\n                f1s.append(f1_score(y_true, y_pred, average='macro'))\n\n            end = time.time()\n            total_minutes = (end - start) / 60\n\n            # المتوسط والانحراف\n            mean_acc = np.mean(accs)\n            std_acc = np.std(accs)\n\n            mean_prec = np.mean(precisions)\n            std_prec = np.std(precisions)\n\n            mean_rec = np.mean(recalls)\n            std_rec = np.std(recalls)\n\n            mean_f1 = np.mean(f1s)\n            std_f1 = np.std(f1s)\n\n            print(f\"{act_name}, lr={learning_rate} | \"\n                  f\"Acc: {mean_acc:.4f}±{std_acc:.4f} | \"\n                  f\"Prec: {mean_prec:.4f}±{std_prec:.4f} | \"\n                  f\"Rec: {mean_rec:.4f}±{std_rec:.4f} | \"\n                  f\"F1: {mean_f1:.4f}±{std_f1:.4f} \"\n                  f\"({len(SEEDS)} seeds) | Time: {total_minutes:.2f} min\")\n\n            all_results.append({\n                \"activation\": act_name,\n                \"optimizer\": optimizer_name,\n                \"lr\": learning_rate,\n                \"mean_acc\": mean_acc,\n                \"std_acc\": std_acc,\n                \"mean_prec\": mean_prec,\n                \"std_prec\": std_prec,\n                \"mean_rec\": mean_rec,\n                \"std_rec\": std_rec,\n                \"mean_f1\": mean_f1,\n                \"std_f1\": std_f1,\n                \"all_accs\": accs,\n                \"all_precs\": precisions,\n                \"all_recs\": recalls,\n                \"all_f1s\": f1s\n            })\n\n\n# ============================================================\n# Aggregate results across runs and convert to DataFrame\n# ============================================================\n\ndf_results = pd.DataFrame(all_results)\nprint(\"\\n=== Summary Results ===\")\nprint(df_results[[\n    \"activation\",\"lr\",\n    \"mean_acc\",\"std_acc\",\n    \"mean_prec\",\"std_prec\",\n    \"mean_rec\",\"std_rec\",\n    \"mean_f1\",\"std_f1\"\n]])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# ============================================================\n# Statistical significance analysis\n# Friedman test followed by post-hoc Wilcoxon test\n# ============================================================","metadata":{}},{"cell_type":"code","source":"\nactivations = [\"MishRelU\", \"ReLU\", \"Mish\", \"Elu\", \"LeakyReLU\", \"Selu\"]\nlearning_rates = [0.01, 0.001, 0.0001]\n\nmetrics = [\"all_accs\", \"all_precs\", \"all_recs\", \"all_f1s\"]\n\nfor lr in learning_rates:\n    print(f\"\\n=== Friedman Test for lr = {lr} ===\")\n    for metric in metrics:\n        #\n        values = [df_results.query(f\"activation=='{act}' and lr=={lr}\")[metric].values[0] for act in activations]\n        stat, p = friedmanchisquare(*values)\n        print(f\"{metric.replace('all_', '').capitalize()} -> Friedman statistic = {stat:.4f}, p-value = {p:.6f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#========== Post-hoc Wilcoxon Test ============= ","metadata":{}},{"cell_type":"code","source":"activations = [\"MishRelU\", \"ReLU\", \"Mish\", \"Elu\", \"LeakyReLU\", \"Selu\"]\nmetrics = [\"all_accs\", \"all_precs\", \"all_recs\", \"all_f1s\"]\nlr = 0.0001  \n\ndef get_values(act, metric):\n    return df_results.query(f\"activation=='{act}' and lr=={lr}\")[metric].values[0]\n\nfor metric in metrics:\n    print(\"\\n===================================\")\n    print(\"Post-hoc Wilcoxon Test | Metric:\", metric.replace(\"all_\",\"\").upper())\n    print(\"===================================\\n\")\n\n    # Collect the performance values for each activation function\n    data = {act: get_values(act, metric) for act in activations}\n\n    # Compute the mean performance to rank activation functions\n    avg_scores = {act: np.mean(vals) for act, vals in data.items()}\n\n    print(\"Average Scores:\")\n    for act, score in avg_scores.items():\n        print(f\"{act}: {score:.4f}\")\n\n    # the best function\n    best_act = max(avg_scores, key=avg_scores.get)\n    print(f\"\\n==> BEST activation (based on mean): {best_act}\\n\")\n\n    # Wilcoxon between each pair\n    print(\"Wilcoxon Pairwise p-values:\")\n    for i in range(len(activations)):\n        for j in range(i+1, len(activations)):\n            act1 = activations[i]\n            act2 = activations[j]\n            stat, p = wilcoxon(data[act1], data[act2])\n            print(f\"{act1} vs {act2}: p = {p:.6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}